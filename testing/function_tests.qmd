---
title: "Test Functions"
format: html
---

```{r}
# load packages
#library(psych) package version 2.2.5
#library(MBESS) package version 4.9.1
#library(effsize) package version 0.8.1
#library(WRS2) package version 1.1.-4
#library(TOSTER) package version 0.7.1
#library(CohensdpLibrary) package version 0.5.10
```

## Set up simulated data

```{r}
# set parameters for simulated dataset:
## pretest measures across groups a and b
mu_pre <- 20
sigma_pre <- 5

### pretest measures of group a
n_a_pre <- 30
mu_a_pre <- mu_pre * 1.01
sigma_a_pre <- sigma_pre * 1.01

### pretest measures of group b
n_b_pre <- 20
mu_b_pre <- mu_pre * (2 - mu_a_pre/mu_pre)
sigma_b_pre <- sigma_pre * (2 - sigma_a_pre/sigma_pre)

## posttest measures across groups a and b
mu_post <- mu_pre + (sigma_pre * 0.45)
sigma_post <- sigma_pre * 1.2

### posttest measures of group a
n_a_post <- 30
mu_a_post <- mu_pre + (sigma_pre * 0.8)
sigma_a_post <- sigma_post * 1.15

### posttest measures of group b
n_b_post <- 20
mu_b_post <- mu_post * (2 - mu_a_post/mu_post)
sigma_b_post <- sigma_post * (2 - sigma_a_post/sigma_post)

## correlation between pre and posttest measures
cor_pre_post <- 0.8

#Simulate data set to work with
RNGversion("4.1.0")
set.seed(123)
mat_a <- MASS::mvrnorm(n = n_a_pre + n_a_post,
                       mu = c(mu_a_pre, mu_a_post),
                       Sigma = matrix(c(sigma_a_pre^2, sigma_a_pre * sigma_a_post * cor_pre_post,
                                        sigma_a_pre * sigma_a_post * cor_pre_post, sigma_a_post^2),
                                      ncol = 2, byrow = TRUE)
)

RNGversion("4.1.0")
set.seed(123)
mat_b <- MASS::mvrnorm(n = n_b_pre + n_b_post,
                       mu = c(mu_b_pre, mu_b_post),
                       Sigma = matrix(c(sigma_b_pre^2, sigma_b_pre * sigma_b_post * cor_pre_post,
                                        sigma_b_pre * sigma_b_post * cor_pre_post, sigma_b_post^2),
                                      ncol = 2, byrow = TRUE)
)

d <- data.frame(rbind(mat_a, mat_b), 
                factor(
                  rep(c("a", "b"), times = c(n_a_pre + n_a_post, n_b_pre + n_b_post)),
                  levels = c("a", "b"))
)
names(d) <- c("x", "y", "INDEX")

# get relevant parameters from the simulated dataset
m_pre <- mean(d$x)
s_pre <- sd(d$x)
n_pre <- n_a + n_b

m_post <- mean(d$y)
s_post <- sd(d$y)
n_post <- n_a + n_b

m_a_pre <- mean(d$x[d$INDEX == "a"])
s_a_pre <- sd(d$x[d$INDEX == "a"])

m_a_post <- mean(d$y[d$INDEX == "a"])
s_a_post <- sd(d$y[d$INDEX == "a"])

n_a <- n_a_pre + n_a_post

m_b_pre <- mean(d$x[d$INDEX == "b"])
s_b_pre <- sd(d$x[d$INDEX == "b"])

m_b_post <- mean(d$y[d$INDEX == "b"])
s_b_post <- sd(d$y[d$INDEX == "b"])

n_b <- n_b_pre + n_b_post

r <- cor(d$x, d$y)
r_a <- cor(d$x[d$INDEX == "a"], d$y[d$INDEX == "a"])
r_b <- cor(d$x[d$INDEX == "b"], d$y[d$INDEX == "b"])
sdiff_a <- sd(d$y[d$INDEX == "a"] - d$x[d$INDEX == "a"])
sdiff_b <- sd(d$y[d$INDEX == "b"] - d$x[d$INDEX == "b"])
n <- n_pre


# set various parameters necessary for some of the tested functions
alpha <- 0.05
trim <- 0.2
cutoff <- 15
tail <- "lower"
reference_group <- "group2"
```

## Tests

### Cohen's *d*

#### Independent Groups (IG) Design

Test of the point estimator.
```{r}
fun_val <- cohens_d(x = d$y, INDEX = d$INDEX)
test_fun_val <- psych::cohen.d(y ~ INDEX, data = d)[["hedges.g"]][, "effect"]
all.equal(fun_val, test_fun_val)
```

Test of the confidence interval (CI) estimator.
```{r}
fun_val <- unname(
  unlist(
    cohens_d_ci(x = d$y, INDEX = d$INDEX, alpha = 0.05)
  )
)

test_fun_val <- -1 * rev(
  unname(
    unlist(
      TOSTER::smd_calc(formula = y ~ INDEX, 
                       data = d, 
                       paired = FALSE, 
                       var.equal = TRUE, 
                       alpha = 0.05, 
                       bias_correction = FALSE, 
                       smd_ci = "nct")[, c("lower.ci", "upper.ci")]
    )
  )
)

all.equal(fun_val, test_fun_val)
```

#### Dependent Groups (DG) Design

Test of the point estimator.

```{r}
fun_val <- cohens_d(x = d$x, y = d$y)

test_fun_val <- TOSTER::smd_calc(
  formula = X ~ INDEX, 
  data = data.frame(
    X = c(d$x, d$y),
    INDEX = rep(c("b", "a"), each = n)
  ), 
  paired = FALSE, 
  var.equal = TRUE, 
  alpha = 0.05, 
  bias_correction = FALSE)[["estimate"]]

all.equal(fun_val, test_fun_val)
```

Test of the CI estimator. Test intervals differ somewhat since `cohens_d_dependent_ci` and `CohensdpLibrary::Cohensdp` use different pivotal distributions to compute CI. While `cohens_d_dependent_ci` uses the noncentral t-distribution, `CohensdpLibrary::Cohensdp` employs the newly discovered exact distribution of Cohen's d for the DG design.
```{r}
fun_val <- unname(unlist(cohens_d_dependent_ci(x = d$x, y = d$y, alpha = 0.05)))
test_fun_val <- CohensdpLibrary::Cohensdp(
  statistics = list(
    m2 = m_pre, 
    m1 = m_post, 
    s2 = s_pre, 
    s1 = s_post, 
    n = n, 
    r = r), 
  design = "within", 
  gamma = 0.95, 
  method = "piCI")[["interval"]]
all.equal(fun_val, test_fun_val)
```

### Hedges *g*

#### IG Design

Test of the point estimator. 

```{r}
fun_val <- hedges_g(x = d$y, INDEX = d$INDEX)
test_fun_val <- -1 * TOSTER::smd_calc(formula = y ~ INDEX, 
                                      data = d, 
                                      paired = FALSE, 
                                      var.equal = TRUE, 
                                      alpha = 0.05, 
                                      bias_correction = TRUE)[["estimate"]]
all.equal(fun_val, test_fun_val)
```

Test of the CI estimator. The values of the confidence intervals might differ somewhat since `hedges_g_ci` and `MBESS::ci.smd` use different approaches to construct noncentra-t-distribution based confidence intervals. Namely, `hedges_g_ci` uses the $1 - \alpha/2$ and $\alpha/2$ quantiles of a t-distribution with a noncentrality parameter equal to the sample estimate, while `MBESS::ci.smd` uses the noncentrality parameters of t-distributions, which have the sample estimate of the noncentrality parameter as their $1 - \alpha/2$ and $\alpha/2$ quantiles respectively.
```{r}
fun_val <- unname(
  unlist(
    hedges_g_ci(
      x = d$y, 
      INDEX = d$INDEX
    )
  )
)

test_fun_val <- unname(
  unlist(
    MBESS::ci.smd(smd = hedges_g(x = d$y, INDEX = d$INDEX),
                  n.1 = n_a, n.2 = n_b))[-2]
)

all.equal(fun_val, test_fun_val)
```

#### DG Design

Test of the point estimator. The values differ given that `hedges_g` uses $n - 1$ as the degrees of freedom, whereas `CohensdpLibrary::Hedgesp` uses $2(n - 1)$ as the degrees of freedom. The authors of `CohensdpLibrary::Hedgesp` base their choice on simulations reported in Goulet-Pelletier J, Cousineau D (2018). “A review of effect sizes and their confidence intervals, Part I: The Cohen's d family.” The Quantitative Methods for Psychology, 14(4), 242-265. <https://doi.org/10.20982/tqmp.14.4.p242>

```{r}
#| message = false
fun_val <- hedges_g(x = d$x, y = d$y)
test_fun_val <- CohensdpLibrary::Hedgesgp(
  statistics = list(
    m2 = m_pre, 
    m1 = m_post, 
    s2 = s_pre, 
    s1 = s_post, 
    n = n, 
    r = r), 
  design = "within")[["estimate"]]
all.equal(fun_val, test_fun_val)
```

Test of the CI estimator. We could not identify any existing R function that computes a CI for Hedges g for the DG design. We instead compare our CI estimator to the CI estimator for Cohen d for the DG design. Consequently, the results will differ.

```{r}
fun_val <- unname(
  unlist(
    hedges_g_dependent_ci(x = d$x, y = d$y, alpha = 0.05)
  )
)
test_fun_val <- CohensdpLibrary::Cohensdp(
  statistics = list(
    m2 = m_pre, 
    m1 = m_post, 
    s2 = s_pre, 
    s1 = s_post, 
    n = n, 
    r = r), 
  design = "within", 
  gamma = 0.95, 
  method = "piCI")[["interval"]]
all.equal(fun_val, test_fun_val)
```

### Cohen's \(d'\)


#### IG Design

Test of the point estimator.
```{r}
fun_val <- bonett_d(x = d$y, INDEX = d$INDEX)

test_fun_val <- -1 * TOSTER::smd_calc(
  formula = y ~ INDEX, 
  data = d, 
  paired = FALSE, 
  var.equal = FALSE,
  bias_correction = FALSE)[["estimate"]]

all.equal(fun_val, test_fun_val)
```

Test the CI estimator. The functions `bonett_d_ci` and `TOSTER::smd_calc` both compute a CI based on the estimated standard error of the sampling distribution of $d'$. However, the estimators used by the two functions differ:  
* `bonett_d_ci` uses $\widehat{Var}(d') = \frac{d'^2(\frac{s^4_a}{n_a - 1} + \frac{s^4_b}{n_b - 1})}{8s'^4} + \frac{s_a^2}{(n_a-1)(\frac{s_a^2 + s_b^2}{2})^2} + \frac{s_b^2}{(n_b-1)(\frac{s_a^2 + s_b^2}{2})^2}$ based on Bonett, D. G. (2008). Confidence intervals for standardized linear contrasts of means. *Psychological Methods*, *13*(2), 99--109. <https://doi.org/10.1037/1082-989X.13.2.99>.    
* `TOSTER::smd_calc` uses  $\widehat{Var}(d') = \left(\frac{n_a + n_b - 2}{(n_a + n_b - 2) - 2} \cdot \frac{2}{\frac{2 n_a n_b}{n_a + n_b}} \cdot (1 + d'^2 \cdot \frac{\frac{2 n_a n_b}{n_a + n_b}}{2}) \right) - d'^2$.  
This can result in significantly differing intervals.
```{r}
fun_val <- unname(
  unlist(
    bonett_d_ci(x = d$y, 
                INDEX = d$INDEX, 
                alpha = 0.05)
  )
)

test_fun_val <- -1 * rev(
  unname(
    unlist(
      TOSTER::smd_calc(
        formula = y ~ INDEX, 
        data = d, 
        paired = FALSE, 
        var.equal = FALSE, 
        bias_correction = FALSE, 
        alpha = 0.05, 
        smd_ci = "z")[, c("lower.ci", "upper.ci")]
    )
  )
)

all.equal(fun_val, test_fun_val)
```

#### DG Design

Test of the point estimator.
```{r}
fun_val <- bonett_d(x = d$x, y = d$y)

test_fun_val <- TOSTER::smd_calc(
  formula = X ~ INDEX, 
  data = data.frame(
    X = c(d$y, d$x),
    INDEX = rep(c("a", "b"), each = n)
  ), 
  paired = FALSE, 
  var.equal = FALSE,
  bias_correction = FALSE)[["estimate"]]

all.equal(fun_val, test_fun_val)
```

We could not identify any existing R function that computes a CI for small-sample bias corrected version of Cohen's $d'$ for the DG design.  

### Hedges \(g'\)

#### IG Design

Test of the point estimator. 

```{r}
fun_val <- bonett_d_corr(x = d$y, INDEX = d$INDEX)

test_fun_val <- -1 * TOSTER::smd_calc(
  formula = y ~ INDEX, 
  data = d, 
  paired = FALSE, 
  var.equal = FALSE,
  bias_correction = TRUE)[["estimate"]]

all.equal(fun_val, test_fun_val)
```

Test the CI estimator. The functions `bonett_d_corr_ci` and `TOSTER::smd_calc` both compute a CI based on the estimated standard error of the sampling distribution of $d'$. However, the estimators used by the two functions differ:  
* `bonett_d_corr_ci` uses $\widehat{Var}(d') = \left( \frac{d'^2(\frac{s^4_a}{n_a - 1} + \frac{s^4_b}{n_b - 1})}{8s'^4} + \frac{s_a^2}{(n_a-1)(\frac{s_a^2 + s_b^2}{2})^2} + \frac{s_b^2}{(n_b-1)(\frac{s_a^2 + s_b^2}{2})^2} \right) \cdot J(n_a + n_b - 2)$ based on Bonett, D. G. (2008). Confidence intervals for standardized linear contrasts of means. *Psychological Methods*, *13*(2), 99--109. <https://doi.org/10.1037/1082-989X.13.2.99>.    
* `TOSTER::smd_calc` uses  $\widehat{Var}(d') = \left(\frac{n_a + n_b - 2}{(n_a + n_b - 2) - 2} \cdot \frac{2}{\frac{2 n_a n_b}{n_a + n_b}} \cdot (1 + d'^2 \cdot \frac{\frac{2 n_a n_b}{n_a + n_b}}{2}) \right) - \frac{d'^2}{J(n_a + n_b - 2)^2}$.  
This can result in significantly differing intervals.
```{r}
fun_val <- unname(
  unlist(
    bonett_d_corr_ci(
      x = d$y, 
      INDEX = d$INDEX,
      alpha = 0.05
    )
  )
)

test_fun_val <- as.numeric(
  rev(
    -1 * TOSTER::smd_calc(
      formula = y ~ INDEX, 
      data = d, 
      paired = FALSE, 
      var.equal = FALSE,
      bias_correction = TRUE,
      alpha = 0.05)[, c("lower.ci", "upper.ci")]
  )
)

all.equal(fun_val, test_fun_val)
```

#### DG Design

Test of the point estimator. The two functions use different correction factors:
* `bonett_d_corr_dependent` uses $\sqrt{\frac{n - 2}{n - 1}}$ as the correction factor based on Bonett, D. G. (2008). Confidence intervals for standardized linear contrasts of means. *Psychological Methods*, *13*(2), 99--109.   <https://doi.org/10.1037/1082-989X.13.2.99>, 
* whereas `TOSTER::smd_calc` here uses the Hedges correction with $2(n - 1)$ as the degrees of freedom.
This leads to slightly differing values.
```{r}
fun_val <- bonett_d_corr_dependent(x = d$x, y = d$y)

test_fun_val <- TOSTER::smd_calc(
  formula = X ~ INDEX, 
  data = data.frame(
    X = c(d$y, d$x),
    INDEX = rep(c("a", "b"), each = n)
  ), 
  paired = FALSE, 
  var.equal = FALSE,
  bias_correction = TRUE)[["estimate"]]

all.equal(fun_val, test_fun_val)
```

We could not identify any existing R function that computes a CI for small-sample bias corrected version of Cohen's $d'$ for the DG design.

### Kulinskaya-Staudte's \(d^2_{KS}\)

No implementation of this estimator or of its CI could be identified.

### Glass \(d_G\)

#### IG Design

Test the point estimator.
```{r}
fun_val <- glass_d(
  x = d$y, 
  INDEX = d$INDEX, 
  standardised_by_group_1 = TRUE
)

test_fun_val <- -1 * TOSTER::smd_calc(
  formula = y ~ INDEX, 
  data = d, 
  paired = FALSE, 
  var.equal = FALSE,
  glass = "glass1",
  bias_correction = FALSE)[["estimate"]]

all.equal(fun_val, test_fun_val)
```

Test the CI estimator. The function `glass_d_ci` and `TOSTER::smd_calc` both compute a CI based on the noncentral t-distribution. However, they differ in two regards:  
* `glass_d_ci` uses $df = n_j - 1$ as its degrees of freedom for finding a CI for the estimated noncentrality parameter, while `TOSTER::smd_calc` uses $df = n_a + n_b - 2$.  
* `glass_d_ci` multiplies the CI for the estimated noncentrality parameter with the same factor it divides $d_G$ by to get the sample estimate of the noncentrality parameter---that is $c = \sqrt{\frac{1}{n_j} + \frac{s_i^2}{n_i \cdot s_j^2}}$ with $s_j$ and $n_j$ being the sample size and the standard deviation of the group which is used for standardising the mean difference. `TOSTER::smd_calc` on the other hand uses $c = \sqrt{\frac{1}{n_a} + \frac{1}{n_b}}}$.  
This can result in significantly differing interval widths.
```{r}
fun_val <- unname(
  unlist(
    glass_d_ci(
      x = d$y, 
      INDEX = d$INDEX, 
      standardised_by_group_1 = TRUE,
      alpha = 0.05
    )
  )
)

test_fun_val <- as.numeric(
  rev(
    -1 * TOSTER::smd_calc(
      formula = y ~ INDEX, 
      data = d, 
      paired = FALSE, 
      var.equal = FALSE,
      glass = "glass1",
      bias_correction = FALSE,
      smd_ci = "nct",
      alpha = 0.05)[, c("lower.ci", "upper.ci")]
  )
)

all.equal(fun_val, test_fun_val)
```

Testing the point estimator.
```{r}
fun_val <- glass_d(
  x = d$y, 
  INDEX = d$INDEX, 
  standardised_by_group_1 = FALSE
)

test_fun_val <- -1 * TOSTER::smd_calc(
  formula = y ~ INDEX, 
  data = d, 
  paired = FALSE, 
  var.equal = FALSE,
  glass = "glass2",
  bias_correction = FALSE)[["estimate"]]

all.equal(fun_val, test_fun_val)
```

Test the CI estimator. The function `glass_d_dependent_ci` and `TOSTER::smd_calc` both compute a CI based on the estimated standard error of the sampling distribution of $d_{G, \, j}$. However, the estimators used by the two functions differ:  
* `glass_d_dependent_ci` uses $\widehat{Var}(d_{G,\,j}) = \frac{d^2_{Gj}}{2(n - 1)} + \frac{s^2_d}{s^2_j(n - 1)}$ (with $s_d$ being the standard deviation of difference scores).  
* `TOSTER::smd_calc` uses  $\widehat{Var}(d_{G,\,j}) = \frac{d^2_{Gj}}{2n} + \frac{1 - r^2}{n}$ (with $r$ being the correlation of the dependent observartions).  
This can result in significantly differing intervals.
```{r}
fun_val <- unname(
  unlist(
    glass_d_ci(
      x = d$y, 
      INDEX = d$INDEX, 
      standardised_by_group_1 = FALSE,
      alpha = 0.05
    )
  )
)
test_fun_val <- as.numeric(
  rev(
    -1 * TOSTER::smd_calc(
      formula = y ~ INDEX, 
      data = d, 
      paired = FALSE, 
      var.equal = FALSE,
      glass = "glass2",
      bias_correction = FALSE,
      smd_ci = "nct",
      alpha = 0.05)[, c("lower.ci", "upper.ci")]
  )
)

all.equal(fun_val, test_fun_val)
```

#### DG Design

Test the point estimator.
```{r}
fun_val <- glass_d(
  x = d$x, 
  y = d$y, 
  standardised_by_group_1 = TRUE
)

test_fun_val <- -1 * TOSTER::smd_calc(
  x = d$x,
  y = d$y, 
  paired = TRUE, 
  var.equal = FALSE,
  glass = "glass1",
  bias_correction = FALSE)[["estimate"]]

all.equal(fun_val, test_fun_val)
```

Test the CI estimator. The function `glass_d_dependent_ci` and `TOSTER::smd_calc` both compute a CI based on the estimated standard error of the sampling distribution of $d_{G, \, j}$. However, the estimators used by the two functions differ:  
* `glass_d_dependent_ci` uses $\widehat{Var}(d_{G,\,j}) = \frac{d^2_{Gj}}{2(n - 1)} + \frac{s^2_d}{s^2_j(n - 1)}$ (with $s_d$ being the standard deviation of difference scores) based on Bonett, D. G. (2015). Interval estimation of standardized mean differences in paired-samples designs. *Journal of Educational and Behavioral Statistics*, *40*(4), 366--376. <https://doi.org/10.3102/1076998615583904>.  
* `TOSTER::smd_calc` uses  $\widehat{Var}(d_{G,\,j}) = \frac{d^2_{G,\,j}}{2n} + \frac{1 - r^2}{n}$ (with $r$ being the correlation of the dependent observartions).  
This can result in significantly differing intervals.
```{r}
fun_val <- unname(
  unlist(
    glass_d_dependent_ci(
      x = d$x, 
      y = d$y, 
      standardised_by_group_1 = TRUE,
      alpha = 0.05)
  )
)

test_fun_val <- as.numeric(
  rev(
    -1 * TOSTER::smd_calc(
      x = d$x,
      y = d$y,
      paired = TRUE, 
      var.equal = FALSE,
      glass = "glass1",
      bias_correction = FALSE,
      smd_ci = "z",
      alpha = 0.05)[, c("lower.ci", "upper.ci")]
  )
)

all.equal(fun_val, test_fun_val)
```

Testing the point estimator.
```{r}
fun_val <- glass_d(
  x = d$x, 
  y = d$y, 
  standardised_by_group_1 = FALSE
)

test_fun_val <- -1 * TOSTER::smd_calc(
  x = d$x,
  y = d$y, 
  paired = TRUE, 
  var.equal = FALSE,
  glass = "glass2",
  bias_correction = FALSE)[["estimate"]]

all.equal(fun_val, test_fun_val)
```

Test the CI estimator. The function `glass_d_dependent_ci` and `TOSTER::smd_calc` both compute a CI based on the estimated standard error of the sampling distribution of $d_{G, \, j}$. However, the estimators used by the two functions differ:  
* `glass_d_dependent_ci` uses $\widehat{Var}(d_{G,\,j}) = \frac{d^2_{Gj}}{2(n - 1)} + \frac{s^2_d}{s^2_j(n - 1)}$ (with $s_d$ being the standard deviation of difference scores) based on Bonett, D. G. (2015). Interval estimation of standardized mean differences in paired-samples designs. *Journal of Educational and Behavioral Statistics*, *40*(4), 366--376. <https://doi.org/10.3102/1076998615583904>.  
* `TOSTER::smd_calc` uses  $\widehat{Var}(d_{G,\,j}) = \frac{d^2_{G,\,j}}{2n} + \frac{1 - r^2}{n}$ (with $r$ being the correlation of the dependent observartions).  
This can result in significantly differing intervals.
```{r}
fun_val <- unname(
  unlist(
    glass_d_dependent_ci(
      x = d$x,
      y = d$y,
      standardised_by_group_1 = FALSE,
      alpha = 0.05
    )
  )
)
test_fun_val <- as.numeric(
  rev(
    -1 * TOSTER::smd_calc(
      x = d$x,
      y = d$y,
      paired = TRUE, 
      var.equal = FALSE,
      glass = "glass2",
      bias_correction = FALSE,
      smd_ci = "z",
      alpha = 0.05)[, c("lower.ci", "upper.ci")]
  )
)

all.equal(fun_val, test_fun_val)
```

### Hedges \(g_G\)

#### IG Design

Test the point estimator.
```{r}
fun_val <- glass_d_corr(
  x = d$y, 
  INDEX = d$INDEX, 
  standardised_by_group_1 = TRUE
)

test_fun_val <- -1 * TOSTER::smd_calc(
  formula = y ~ INDEX, 
  data = d, 
  paired = FALSE, 
  var.equal = FALSE,
  glass = "glass1",
  bias_correction = TRUE)[["estimate"]]

all.equal(fun_val, test_fun_val)
```

Test the CI estimator. The function `glass_d_corr_ci` and `TOSTER::smd_calc` both compute a CI based on the noncentral t-distribution. However, they differ in three regards:  
* `glass_d_corr_ci` uses $df = n_j - 1$ as its degrees of freedom for finding a CI for the estimated noncentrality parameter, while `TOSTER::smd_calc` uses $df = n_a + n_b - 2$.  
* `glass_d_corr_ci` multiplies the CI for the estimated noncentrality parameter with the same factor it divides $g_G$ by to get the sample estimate of the noncentrality parameter---that is $c = \sqrt{\frac{1}{n_j} + \frac{s_i^2}{n_i \cdot s_j^2}}$ with $s_j$ and $n_j$ being the sample size and the standard deviation of the group which is used for standardising the mean difference. `TOSTER::smd_calc` on the other hand uses $c = \sqrt{\frac{1}{n_a} + \frac{1}{n_b}}}$.  
* `glass_d_corr_ci` transform $g_{G, \, j}$ into an estimate of the noncentrality parameter based on the factor given above and computes the CI for $g_{G, \, j}$ by transforming the CI for the estimated noncentrality parameter back into the unit of $g_{G, \, j}$. On the other hand `TOSTER::smd_calc` transforms $d_{G, \, j}$ into an estimate of the noncentrality parameter and then transforms the computed CI for $d_{G, \, j}$ into a CI for $g_{G, \, j}$ by applying the Hedges correction.  
This can result in significantly differing interval widths.
```{r}
fun_val <- unname(
  unlist(
    glass_d_corr_ci(
      x = d$y, 
      INDEX = d$INDEX, 
      standardised_by_group_1 = TRUE,
      alpha = 0.05
    )
  )
)

test_fun_val <- as.numeric(
  rev(
    -1 * TOSTER::smd_calc(
      formula = y ~ INDEX, 
      data = d, 
      paired = FALSE, 
      var.equal = FALSE,
      glass = "glass1",
      bias_correction = TRUE,
      smd_ci = "nct",
      alpha = 0.05)[, c("lower.ci", "upper.ci")]
  )
)

all.equal(fun_val, test_fun_val)
```

Testing the point estimator.
```{r}
fun_val <- glass_d_corr(
  x = d$y, 
  INDEX = d$INDEX, 
  standardised_by_group_1 = FALSE
)

test_fun_val <- -1 * TOSTER::smd_calc(
  formula = y ~ INDEX, 
  data = d, 
  paired = FALSE, 
  var.equal = FALSE,
  glass = "glass2",
  bias_correction = TRUE)[["estimate"]]

all.equal(fun_val, test_fun_val)
```

Test the CI estimator. The function `glass_d_corr_ci` and `TOSTER::smd_calc` both compute a CI based on the noncentral t-distribution. However, they differ in three regards:  
* `glass_d_corr_ci` uses $df = n_j - 1$ as its degrees of freedom for finding a CI for the estimated noncentrality parameter, while `TOSTER::smd_calc` uses $df = n_a + n_b - 2$.  
* `glass_d_corr_ci` multiplies the CI for the estimated noncentrality parameter with the same factor it divides $g_G$ by to get the sample estimate of the noncentrality parameter---that is $c = \sqrt{\frac{1}{n_j} + \frac{s_i^2}{n_i \cdot s_j^2}}$ with $s_j$ and $n_j$ being the sample size and the standard deviation of the group which is used for standardising the mean difference. `TOSTER::smd_calc` on the other hand uses $c = \sqrt{\frac{1}{n_a} + \frac{1}{n_b}}}$.  
* `glass_d_corr_ci` transform $g_{G, \, j}$ into an estimate of the noncentrality parameter based on the factor given above and computes the CI for $g_{G, \, j}$ by transforming the CI for the estimated noncentrality parameter back into the unit of $g_{G, \, j}$. On the other hand `TOSTER::smd_calc` transforms $d_{G, \, j}$ into an estimate of the noncentrality parameter and then transforms the computed CI for $d_{G, \, j}$ into a CI for $g_{G, \, j}$ by applying the Hedges correction.  
This can result in significantly differing interval widths.
```{r}
fun_val <- unname(
  unlist(
    glass_d_corr_ci(
      x = d$y, 
      INDEX = d$INDEX, 
      standardised_by_group_1 = FALSE,
      alpha = 0.05
    )
  )
)

test_fun_val <- as.numeric(
  rev(
    -1 * TOSTER::smd_calc(
      formula = y ~ INDEX, 
      data = d, 
      paired = FALSE, 
      var.equal = FALSE,
      glass = "glass2",
      bias_correction = TRUE,
      smd_ci = "nct",
      alpha = 0.05)[, c("lower.ci", "upper.ci")]
  )
)

all.equal(fun_val, test_fun_val)
```


#### DG Design

Test the point estimator.
```{r}
fun_val <- glass_d_corr(
  x = d$x, 
  y = d$y, 
  standardised_by_group_1 = TRUE
)

test_fun_val <- -1 * TOSTER::smd_calc(
  x = d$x,
  y = d$y, 
  paired = FALSE, 
  var.equal = FALSE,
  glass = "glass1",
  bias_correction = TRUE)[["estimate"]]

all.equal(fun_val, test_fun_val)
```

Test the CI estimator. The function `glass_d_corr_dependent_ci` and `TOSTER::smd_calc` both compute a CI based on the estimated standard error of the sampling distribution of $d_{G, \, j}$. However, the estimators used by the two functions differ:  
* `glass_d_corr_dependent_ci` uses $\widehat{Var}(g_{G,\,j}) = J(n - 1)^2 \left( \frac{d^2_{Gj}}{2(n - 1)} + \frac{s^2_d}{s^2_j(n - 1)} \right)$ (with $J( \cdot )$ being the Hedges correction.  
* `TOSTER::smd_calc` uses  $\widehat{Var}(g_{G,\,j}) =J(n - 1)^2 \left( \frac{d^2_{G, \, j}}{2nJ(n - 1)} + \frac{1 - r^2}{n} \left)$ (with $r$ being the correlation of the dependent observartions).  
This can result in significantly differing intervals.
```{r}
fun_val <- unname(
  unlist(
    glass_d_corr_dependent_ci(
      x = d$x, 
      y = d$y, 
      standardised_by_group_1 = TRUE,
      alpha = 0.05)
  )
)

test_fun_val <- as.numeric(
  rev(
    -1 * TOSTER::smd_calc(
      x = d$x,
      y = d$y,
      paired = FALSE, 
      var.equal = FALSE,
      glass = "glass1",
      bias_correction = TRUE,
      smd_ci = "z",
      alpha = 0.05)[, c("lower.ci", "upper.ci")]
  )
)

all.equal(fun_val, test_fun_val)
```

Testing the point estimator.
```{r}
fun_val <- glass_d_corr(
  x = d$x, 
  y = d$y, 
  standardised_by_group_1 = FALSE
)

test_fun_val <- -1 * TOSTER::smd_calc(
  x = d$x,
  y = d$y, 
  paired = FALSE, 
  var.equal = FALSE,
  glass = "glass2",
  bias_correction = TRUE)[["estimate"]]

all.equal(fun_val, test_fun_val)
```

Test the CI estimator. The function `glass_d_corr_dependent_ci` and `TOSTER::smd_calc` both compute a CI based on the estimated standard error of the sampling distribution of $d_{G, \, j}$. However, the estimators used by the two functions differ:  
* `glass_d_corr_dependent_ci` uses $\widehat{Var}(g_{G,\,j}) = J(n - 1)^2 \left( \frac{d^2_{Gj}}{2(n - 1)} + \frac{s^2_d}{s^2_j(n - 1)} \right)$ (with $J( \cdot )$ being the Hedges correction.  
* `TOSTER::smd_calc` uses  $\widehat{Var}(g_{G,\,j}) =J(n - 1)^2 \left( \frac{d^2_{G, \, j}}{2nJ(n - 1)} + \frac{1 - r^2}{n} \left)$ (with $r$ being the correlation of the dependent observartions).  
This can result in significantly differing intervals.
```{r}
fun_val <- unname(
  unlist(
    glass_d_corr_dependent_ci(
      x = d$x,
      y = d$y,
      standardised_by_group_1 = FALSE,
      alpha = 0.05
    )
  )
)

test_fun_val <- as.numeric(
  rev(
    -1 * TOSTER::smd_calc(
      x = d$x,
      y = d$y,
      paired = FALSE, 
      var.equal = FALSE,
      glass = "glass2",
      bias_correction = TRUE,
      smd_ci = "z",
      alpha = 0.05)[, c("lower.ci", "upper.ci")]
  )
)

all.equal(fun_val, test_fun_val)
```


### Cohen's \(d_{RM}\)

Test the point estimator.
```{r}
fun_val <- cohens_drm(x = d$x, y = d$y)

test_fun_val <- -1 * TOSTER::smd_calc(
  x = d$x,
  y = d$y,
  bias_correction = FALSE,
  paired = TRUE,
  rm_correction = TRUE
)[["estimate"]]

all.equal(fun_val, test_fun_val)
```

Test the CI estimator. The functions `cohens_drm_ci` and `TOSTER::smd_calc` both compute a CI based on the estimated standard error of the sampling distribution of $d_{G, \, j}$. However, the estimators used by the two functions differ:  
* `cohens_drm_ci` uses $\widehat{Var}(d_{RM}) = 2(1-r)(\frac{d^2_{RM}}{2n} + \frac{1}{n})$ based on Bonett, D. G. (2015). Interval estimation of standardized mean differences in paired-samples designs. *Journal of Educational and Behavioral Statistics*, *40*(4), 366--376. <https://doi.org/10.3102/1076998615583904>.  
* `TOSTER::smd_calc` uses  $\widehat{Var}(d_{RM}) = \left(\frac{n - 1}{(n - 1) - 2} \cdot \frac{2(1 - r)}{n} \cdot (1 + d_{RM}^2 \cdot \frac{n}{2(1 - r)}) \right) - d_{RM}^2$ (with $r$ being the correlation of the dependent observartions).  
This can result in significantly differing intervals.
```{r}
fun_val <- unlist(
  unname(
    cohens_drm_ci(x = d$x, y = d$y)
  )
)
test_fun_val <- as.numeric(
  rev(
    -1 * TOSTER::smd_calc(
      x = d$x,
      y = d$y,
      bias_correction = FALSE,
      paired = TRUE,
      rm_correction = TRUE,
      smd_ci = "z",
      alpha = 0.05
    )[, c("lower.ci", "upper.ci")]
  )
)

all.equal(fun_val, test_fun_val)
```


### Hedges \(g_{RM}\)

Test the point estimator.
```{r}
fun_val <- hedges_grm(x = d$x, y = d$y)

test_fun_val <- -1 * TOSTER::smd_calc(
  x = d$x,
  y = d$y,
  bias_correction = TRUE,
  paired = TRUE,
  rm_correction = TRUE
)[["estimate"]]

all.equal(fun_val, test_fun_val)
```

Test the CI estimator. The functions `hedges_grm_ci` and `TOSTER::smd_calc` both compute a CI based on the estimated standard error of the sampling distribution of $d_{G, \, j}$. However, the estimators used by the two functions differ:  
* `hedges_grm_ci` uses $\widehat{Var}(g_{RM}) = J(n-1)^2 \cdot 2(1-r) \cdot(\frac{d^2_{RM}}{2n} + \frac{1}{n})$ based on Bonett, D. G. (2015). Interval estimation of standardized mean differences in paired-samples designs. *Journal of Educational and Behavioral Statistics*, *40*(4), 366--376. <https://doi.org/10.3102/1076998615583904>.  
* `TOSTER::smd_calc` uses  $\widehat{Var}(d_{RM}) = \left(\frac{n - 1}{(n - 1) - 2} \cdot \frac{2(1 - r)}{n} \cdot (1 + d_{RM}^2 \cdot \frac{n}{2(1 - r)}) \right) - \frac{d_{RM}^2}{J(n - 1)^2}$ (with $r$ being the correlation of the dependent observartions).  
This can result in significantly differing intervals.
```{r}
fun_val <- unlist(
  unname(
    hedges_grm_ci(x = d$x, y = d$y)
  )
)

test_fun_val <- as.numeric(
  rev(
    -1 * TOSTER::smd_calc(
      x = d$x,
      y = d$y,
      bias_correction = TRUE,
      paired = TRUE,
      rm_correction = TRUE,
      smd_ci = "z",
      alpha = 0.05
    )[, c("lower.ci", "upper.ci")]
  )
)

all.equal(fun_val, test_fun_val)
```

### Cohen's \(d_z\)

Test the point estimator.
```{r}
fun_val <- cohens_dz(x = d$x, y = d$y)

test_fun_val <- -1 * TOSTER::smd_calc(
  x = d$x,
  y = d$y,
  bias_correction = FALSE,
  paired = TRUE,
  rm_correction = FALSE
)[["estimate"]]

all.equal(fun_val, test_fun_val)
```

Test the CI estimator.
```{r}
fun_val <- unlist(
  unname(
    cohens_dz_ci(x = d$x, y = d$y, alpha = 0.05)
  )
)

test_fun_val <- as.numeric(
  rev(
    -1 * TOSTER::smd_calc(
      x = d$x,
      y = d$y,
      bias_correction = FALSE,
      paired = TRUE,
      rm_correction = FALSE,
      smd_ci = "nct",
      alpha = 0.05
    )[, c("lower.ci", "upper.ci")]
  )
)

all.equal(fun_val, test_fun_val)
```

### Hedges \(g_z\)

Test the point estimator.
```{r}
fun_val <- hedges_gz(x = d$x, y = d$y)

test_fun_val <- -1 * TOSTER::smd_calc(
  x = d$x,
  y = d$y,
  bias_correction = TRUE,
  paired = TRUE,
  rm_correction = FALSE
)[["estimate"]]

all.equal(fun_val, test_fun_val)
```

Test the CI estimator. The functions `hedges_gz_ci` and `TOSTER::smd_calc` both compute a noncentral t-distribution based CI. However, the estimators used by the two functions differ slightly: `hedges_gz_ci` transforms the sample estimate of $g_z$ into an estimate of the noncentrality parameter by multiplying it with the factor $c = \frac{1}{\sqrt{n}}$ and then transforms the CI for the noncentrality parameter back into the $g_z$ metric by dividing the CI limits by the factor $c$.On the other hand `TOSTER::smd_calc` transforms $d_z$ into an estimate of the noncentrality parameter by multiplying it with the factor $c$ and then transforms the computed CI for $d_z}$ into a CI for $g_z$ by applying the Hedges correction.  
This can result in significantly differing intervals.
```{r}
fun_val <- unlist(
  unname(
    hedges_gz_ci(x = d$x, y = d$y, alpha = 0.05)
  )
)

test_fun_val <- as.numeric(
  rev(
    -1 * TOSTER::smd_calc(
      x = d$x,
      y = d$y,
      bias_correction = TRUE,
      paired = TRUE,
      rm_correction = FALSE,
      smd_ci = "nct",
      alpha = 0.05
    )[, c("lower.ci", "upper.ci")]
  )
)

all.equal(fun_val, test_fun_val)
```



### Cohen's \(d_R\)

#### IG Design

Test the point estimator.
```{r}
fun_val <- robust_cohens_d(x = d$y, INDEX = d$INDEX)

test_fun_val <- WRS2::akp.effect(
  formula = d$y ~ factor(d$INDEX, levels = c("b", "a")),
  EQVAR = TRUE,
  tr = 0.2)[["AKPeffect"]]

all.equal(fun_val, test_fun_val)
```

#### DG Design

Test the point estimator.
```{r}
fun_val <- robust_cohens_d(x = d$x, y = d$y)

test_fun_val <- WRS2::akp.effect(
  formula = c(d$x, d$y) ~ rep(c("b", "a"), each = n_a + n_b),
  EQVAR = TRUE,
  tr = 0.2)[["AKPeffect"]]

all.equal(fun_val, test_fun_val)
```

### Cohen's \(d'_R\)

#### IG design

No implementation of this estimator or of its CI could be identified.

#### DG design

No implementation of this estimator or of its CI could be identified.

### Glass \(d_{R,\,j}\)

#### IG Design

Estimating $\Delta_{R,\,a} = c \cdot \frac{\mu_{t,\,a} - \mu_{t,\,b}}{\sigma_{w,\,a}}$ (see documentation of this function)
```{r}
fun_val <- robust_glass_d(
  x = d$y, INDEX = 
    d$INDEX, 
  standardised_by_group_1 = FALSE)
test_fun_val <- WRS2::akp.effect(
  formula = d$y ~ factor(d$INDEX, levels = c("b", "a")), 
  EQVAR = FALSE,
  tr = 0.2)[["AKPeffect"]]

all.equal(fun_val, test_fun_val)
```

Estimating $\Delta_{R,\,b} = c \cdot \frac{\mu_{t,\,a} - \mu_{t,\,b}}{\sigma_{w,\,b}}$ (see documentation of this function)
```{r}
fun_val <- robust_glass_d(
  x = d$y, 
  INDEX = d$INDEX, 
  standardised_by_group_1 = TRUE)

test_fun_val <- -1 * WRS2::akp.effect(
  formula = d$y ~ d$INDEX, 
  EQVAR = FALSE,
  tr = 0.2)[["AKPeffect"]]

all.equal(fun_val, test_fun_val)
```

No implementation of the CI for this estimator could be identified.

#### DG Design

Estimating $\Delta_{R,\,a} = c \cdot \frac{\mu_{t,\,a} - \mu_{t,\,b}}{\sigma_{w,\,a}}$ (see documentation of this function)
```{r}
fun_val <- robust_glass_d(
  x = d$x, 
  y = d$y, 
  standardised_by_group_1 = FALSE)

test_fun_val <- WRS2::akp.effect(
  formula = c(d$x, d$y) ~ rep(c("b", "a"), each = n_a + n_b), 
  EQVAR = FALSE,
  tr = 0.2)[["AKPeffect"]]

all.equal(fun_val, test_fun_val)
```

Estimating $\Delta_{R,\,b} = c \cdot \frac{\mu_{t,\,a} - \mu_{t,\,b}}{\sigma_{w,\,b}}$ (see documentation of this function)
```{r}
fun_val <- robust_glass_d(
  x = d$x,
  y = d$y, 
  standardised_by_group_1 = TRUE)

test_fun_val <- -1 * WRS2::akp.effect(
  formula = c(d$x, d$y) ~ rep(c("a", "b"), each = n_a + n_b), 
  EQVAR = FALSE,
  tr = 0.2)[["AKPeffect"]]

all.equal(fun_val, test_fun_val)
```

No implementation of the CI for this estimator could be identified.


### Cohen's \(d_{zR})\)

Test the point estimator
```{r}
fun_val <- robust_cohens_dz(d$x, d$y)

test_fun_val <- WRS2:::D.akp.effect(
  d$y, 
  d$x,
  tr = 0.2)

all.equal(fun_val, test_fun_val)
```

No closed form formula for constructing confidence intervals is known for this effect size.



### Variance Ratio

```{r}
fun_val <- unlist(variance_ratio_independent_ci(x = d$x, INDEX = d$INDEX, standardised_by_group_1 = FALSE), use.names = FALSE)
test_fun_val <- stats::var.test(x = d[d$INDEX == "a", "x"], y = d[d$INDEX == "b", "x"])$conf.int
attributes(test_fun_val) <- NULL
all.equal(fun_val, test_fun_val)
```

### \(OVL\)

#### IG Design

##### Parametric Estimation

###### Point Estimator

```{r}
fun_val <- parametric_ovl(x = d$x, INDEX = d$INDEX)
test_fun_val <- effectsize::p_overlap(x = d[d$INDEX == "a", "x"], y = d[d$INDEX == "b", "x"])$Overlap
all.equal(fun_val, test_fun_val)
rm(fun_val, test_fun_val)
```

###### Interval Estimator

The results of `parametric_ovl_ci()` and `effectsize::p_overlap()` differ slightly. The reason for this is that the two functions use slightly different implementations of a non-centrality parameter based CI. While `parametric_ovl_ci()` uses a non-central *F*-distribution, `effectsize::p_overlap()` uses a non-central *t*-distribution. Reiser and Faraggi (1999) found the non-central *F* based CI to be somewhat better performant under the conditions studied by them. However, the differences between the two methods were small for most conditions.
```{r}
fun_val <- unname(unlist(parametric_ovl_ci(x = d$x, INDEX = d$INDEX)))
test_fun_val <- unname(unlist(effectsize::p_overlap(x = d[d$INDEX == "a", "x"], y = d[d$INDEX == "b", "x"])[c('CI_low', 'CI_high')]))
all.equal(fun_val[1], test_fun_val[1])
all.equal(fun_val[2], test_fun_val[2])
rm(fun_val, test_fun_val)
```

##### Non-parametric Estimation

The results of `non_parametric_ov()` and `effectsize::p_overlap(parametric = FALSE)` differ somewhat. The reason for this is that the built in *R* density estimators used by both functions are called with different values for the *bw* argument. The former functions uses the default value of *nrd0* while the latter function uses the "SJ" value. We do not wish to delve into a deep discussion of these function argument values and instead refer the reader to the *R* documentation of the `density()` function.

```{r}
fun_val <- non_parametric_ovl(x = d$x, INDEX = d$INDEX)
test_fun_val <- effectsize::p_overlap(x = d[d$INDEX == "a", "x"], y = d[d$INDEX == "b", "x"], parametric = FALSE)$Overlap[[1]]
all.equal(fun_val, test_fun_val)
rm(fun_val, test_fun_val)
```

#### DG Design

### \(OVL_2\)

#### IG Design

##### Parametric Estimation

```{r}
fun_val <- parametric_ovl_two(x = d$x, INDEX = d$INDEX)
test_fun_val <- 1 - effectsize::cohens_u1(x = d[d$INDEX == "a", "x"], y = d[d$INDEX == "b", "x"])$Cohens_U1
all.equal(fun_val, test_fun_val)
rm(fun_val, test_fun_val)
```

##### Non-parametric Estimation


### Cohen's \(U_1\)

#### IG Design

##### Parametric Estimation

```{r}
fun_val <- parametric_cohens_u1(x = d$x, INDEX = d$INDEX)
test_fun_val <- effectsize::cohens_u1(x = d[d$INDEX == "a", "x"], y = d[d$INDEX == "b", "x"])$Cohens_U1
all.equal(fun_val, test_fun_val)
rm(fun_val, test_fun_val)
```

### Cohen's \(U_2\)

#### IG Design

##### Parametric Estimation

```{r}
fun_val <- parametric_cohens_u2(x = d$x, INDEX = d$INDEX)
test_fun_val <- effectsize::cohens_u2(x = d[d$INDEX == "b", "x"], y = d[d$INDEX == "a", "x"])$Cohens_U2
all.equal(fun_val, test_fun_val)
rm(fun_val, test_fun_val)
```

##### Non-parametric Estimation

```{r}
fun_val <- non_parametric_cohens_u2(x = d$x, INDEX = d$INDEX)
test_fun_val <- effectsize::cohens_u2(x = d[d$INDEX == "a", "x"], y = d[d$INDEX == "b", "x"], parametric = FALSE)$Cohens_U2
all.equal(fun_val, test_fun_val)
rm(fun_val, test_fun_val)
```

### Cohen's \(U_3\)

#### IG Design 

##### Parametric Estimation

```{r}
fun_val <- parametric_cohens_u3(x = d$x, INDEX = d$INDEX)
test_fun_val <- effectsize::cohens_u3(x = d[d$INDEX == "a", "x"], y = d[d$INDEX == "b", "x"])$Cohens_U3
all.equal(fun_val, test_fun_val)
rm(fun_val, test_fun_val)
```

##### Non-parametric Estimation

```{r}
fun_val <- non_parametric_cohens_u3(x = d$x, INDEX = d$INDEX)
test_fun_val <- effectsize::cohens_u3(x = d[d$INDEX == "a", "x"], y = d[d$INDEX == "b", "x"], parametric = FALSE)$Cohens_U3[[1]]
all.equal(fun_val, test_fun_val)
rm(fun_val, test_fun_val)
```

### \(CLES\)

#### IG Design

```{r}
fun_val <- common_language_es(x = d$x, INDEX = d$INDEX)
test_fun_val <- effectsize::p_superiority(x = d[d$INDEX == "b", "x"], y = d[d$INDEX == "a", "x"])$p_superiority
all.equal(fun_val, test_fun_val)
rm(fun_val, test_fun_val)
```

#### DG Design

The results differ between `common_language_es_dependent()` and `effectsize::p_superiority(paired = TRUE)`. While the former transform Cohen's $d_z$ into the *CLES* as $\Phi(d_z)$, the latter transforms Cohen's *dz* into the *CLES* as $\Phi(\frac{d_z}{\sqrt2})$. However, the latter formula was developed for transforming Cohen's *d* into the *CLES* and does not account for the correlation between the contrasted group's measurements.
```{r}
fun_val <- common_language_es_dependent(x = d$x, y = d$y)
test_fun_val <- effectsize::p_superiority(x = d$y, y = d$x, paired = TRUE)$p_superiority
all.equal(fun_val, test_fun_val)
rm(fun_val, test_fun_val)
```

### Probability of Superiority/ The *A* Measure of Stochastic Superiority

#### IG Design

```{r}
fun_val <- mann_whitney_based_ps(x = d$x, INDEX = d$INDEX, ignore_ties = FALSE)
test_fun_val <- effectsize::p_superiority(x = d[d$INDEX == "b", "x"], y = d[d$INDEX == "a", "x"], parametric = FALSE)$p_superiority
all.equal(fun_val, test_fun_val)
rm(fun_val, test_fun_val)
```

#### DG Design

The results differ between `ps_dependent_group()` and `effectsize::p_superiorty(paired = TRUE, parametric = FALSE)`. While the former computes the probability of superiority according to Grissom and Kim (2012) by dividing the number of positive difference scores ( $x_1 - x_2$ ) by the number of pairs of observations (minus ties), the latter transforms the rank biserial correlation of dependent groups into the probability of superiority as $(r_{rb} + 1)/2$. The latter formula was developed for transforming the rank biserial correlation of interdependent groups into the probability of superiority. Therefore, this formula might not be applicable for the dependent samples case.
```{r}
fun_val <- ps_dependent_groups(x = d$x, y = d$y, ignore_ties = FALSE)
test_fun_val <- effectsize::p_superiority(x = d$y, y = d$x, parametric = FALSE, paired = TRUE)$p_superiority
all.equal(fun_val, test_fun_val)
rm(fun_val, test_fun_val)
```

While `ps_dependent_group()` returns the same value for the probability of superiority for dependent groups given for an example data set in Grissom and Kim (2012), `effectsize::p_superiorty(paired = TRUE, parametric = FALSE)` returns a different value.
```{r}
# data from Grissom and Kim (2012, p. 56)
gk <- data.frame(
  y = c(95.2, 94.3, 91.5, 91.9, 100.3, 76.7, 76.8, 101.6, 94.9, 75.2, 77.8, 95.5, 90.7, 92.5, 93.8, 91.7, 98),
  x = c(83.8, 83.3, 86, 82.5, 86.7, 79.6, 76.9, 94.2, 73.4, 80.5, 81.6, 82.1, 77.6, 83.5, 89.9, 86, 87.3)
)
# Grissom and Kim (2012, p. 173) report a probability of superiority of 0.76
# our function returns:
round(ps_dependent_groups(gk$x, gk$y, ignore_ties = TRUE), 2)

# effectsize::p_superioirty returns
round(effectsize::p_superiority(gk$y, gk$x, paired = TRUE, parametric = FALSE)[[1]], 2)

# which is equal to:
round((effectsize:::.r_rbs(gk$y, gk$x, mu = 0, paired = TRUE) + 1)/2, 2)
```


### Dominance Measure

#### IG design

```{r}
fun_val <- dominance_measure_based_es(x = d$x, INDEX = d$INDEX)
test_fun_val <- effectsize::cliffs_delta(x = d[d$INDEX == "b", "x"], y = d[d$INDEX == "a", "x"])[[1]]
all.equal(fun_val, test_fun_val)
rm(fun_val, test_fun_val)
```

#### DG design

No implementation of the dependent samples version of the dominance measure was identified and unfrotunately, the data used by Cliff (1993) to demonstrate the computation of the dominance measure for the dependent groups case is not available. Thus, the correctness of our implementation could not be checked against an established implementation or a value reported in a paper on the effect size.

### Generalized Odds Ratio 

#### IG design

```{r}
fun_val <- generalized_odds_ratio(x = d$x, INDEX = d$INDEX)
test_fun_val <- effectsize::wmw_odds(x = d[d$INDEX == "b", "x"], y = d[d$INDEX == "a", "x"], parametric = FALSE)[[1]]
all.equal(fun_val, test_fun_val)
rm(fun_val, test_fun_val)
```

#### DG design

The results differ between `generalized_odds_ratio()` and `effectsize::wmw_odds(paired = TRUE, parametric = FALSE)`. This makes sense given that the results of `ps_dependent_group()` and `effectsize::p_superiorty(paired = TRUE, parametric = FALSE)` differed too and that the generalized odds ratio is computed as the ratio of the probability of superiority of group one over group two divided by the probability of superiority of group two over group one. If the estimates of these probabilities differ, the ratio will differ too. For a discussion see the above discussion for the dependent groups probability of superiority

```{r}
fun_val <- generalized_odds_ratio(x = d$x, y = d$y)
test_fun_val <- effectsize::wmw_odds(x = d$y, y = d$x, paired = TRUE, parametric = FALSE)[[1]]
all.equal(fun_val, test_fun_val)
rm(fun_val, test_fun_val)
```





```{r}
wsp <- data.frame(
  INDEX = rep(c(0, 1), each = 20),
  x =  c(8, 4, 6, 3, 1, 4, 4, 6, 4, 2, 2, 1, 1, 4, 3, 3, 2, 6, 3, 4, 2, 1, 1, 3, 2, 7, 2, 1, 3, 1, 0, 2, 4, 2, 3, 3, 0, 1, 2, 2)
)

mann_whitney_based_ps(wsp$x, wsp$INDEX, ignore_ties = TRUE)
mann_whitney_based_ps(wsp$x, wsp$INDEX, ignore_ties = FALSE)
effectsize::p_superiority(wsp[wsp==1, "x"], wsp[wsp==0, "x"])$p_superiority
effectsize::vd_a(wsp[wsp==1, "x"], wsp[wsp==0, "x"])$p_superiority
effectsize:::r_rbs()
zps <- -1 * rpb * sqrt(2*(20-1))/sqrt(20*(1-rpb^2))
1-pnorm(zps)
```

